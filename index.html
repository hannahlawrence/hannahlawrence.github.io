<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <title>Hannah Lawrence</title>
  
  <meta name="author" content="Hannah Lawrence">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/MITLogo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hannah Lawrence</name>
              </p>
              <!-- 
              <p>
                Note: this page is under construction.
              </p> -->
              <p>I am a second-year PhD student in machine learning theory at MIT, where I am fortunate to be advised by <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a>. Previously, I was a research analyst at the <a href="https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/">Center for Computational Mathematics</a> of the <a href="https://www.simonsfoundation.org/flatiron/">Flatiron Institute</a> in New York, where I worked on developing algorithms at the interface of signal processing and deep learning for cryoEM. Broadly, I enjoy working on mathematical algorithms for data science and equivariant learning. 
              </p>
              <p>
                I spent summer 2019 at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/">Microsoft Research</a>, where I was lucky to be mentored by <a href=
                "https://people.cs.umass.edu/~cmusco/">Cameron Musco</a> (now at <a href="https://www.umass.edu/">Amherst</a>). I've also spent productive summers at <a href="https://www.reservoir.com/">Reservoir Labs</a> and the <a href="https://www.simonsfoundation.org/flatiron/center-for-computational-biology/">Center for Computational Biology</a>. I was an undergrad at Yale in applied math and computer science, where I had the good fortune of being advised by <a href="https://seas.yale.edu/faculty-research/faculty-directory/amin-karbasi">Amin Karbasi</a> and <a href="http://www.cs.yale.edu/homes/spielman/">Dan Spielman</a>.
                
              </p>
              <p style="text-align:center">
                <a href="mailto:hanlaw@mit.edu">Email</a> &nbsp/&nbsp
                <a href="https://github.com/hannahlawrence/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hannah-lawrence-417b5a130/"> LinkedIn </a> &nbsp/&nbsp
                <!-- <a href="data/CVFall2020_forwebsite.pdf"> CV </a> &nbsp/&nbsp -->
                <a href="https://twitter.com/HLawrenceCS"> Twitter </a> &nbsp&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/HannahLawrence.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/HannahLawrence_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests include symmetry-aware machine learning, signal processing, sparse recovery, and numerical linear algebra, and especially the applications in which some subset of these paradigms intersect. I like developing provably robust, efficient algorithms for inverse problems, sometimes in imaging applications.
                <br />
                <br />
                Here are a few questions I've been thinking about recently:

                <ul>
                  <li> <i> Are there efficient universal approximation for certain subclasses of equivariant functions? </i> </li>
                  <li> <i> How can we automatically identify and interpret coherent patterns of errors made by a trained model? </i> </li>
                  <li> <i> Are there efficient methods for (1) enforcing equivariance w.r.t large groups (given access to a uniform sampler) on linear/kernel learning? (2) enforcing approximate equivariance? </i> </li>
                  <li> <i> Is there a clear theoretical justification for the empirical success of equivariance as an inductive prior in neural architectures? What's the right way to formulate this? </i> </li>

                  <!--
                  <li> When might the minimax-optimal mini-batching algorithm for switching-constrained online convex optimization be useful in practical applications, and what real-world factors beyond switching might we care about? </li>
                  <li>How much hot chocolate can I consume at a single research institution? </li> -->
                </ul>

                And here are some older questions, on the back burner:
                <ul>
                  <li> <i> Given data, can one learn an underlying dictionary if the corresponding coefficients are not sparse, but rather come from a known generative model? </i> </li>

                  <li> <i> When does equivariance with respect to the wreath product group arise in deep learning applications? </i> </li>

                  <li> <i> Is it possible to characterize the class of generative models under which Fourier phase retrieval is well-conditioned? </i> </li>
                  <li> <i> What's the fastest way to rotationally align two spherical functions? </i> </li>
                  <li> <i> What generalizations of (1) the restricted isometry property and (2) leverage score sampling might be useful for off-grid sparse recovery? </i></li>
                  <!--
                  <li> When might the minimax-optimal mini-batching algorithm for switching-constrained online convex optimization be useful in practical applications, and what real-world factors beyond switching might we care about? </li>
                  <li>How much hot chocolate can I consume at a single research institution? </li> -->
                </ul>

              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="gulp_stop()" onmouseover="gulp_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='cauli_image'><img src='images/gulp_width.jpg'></div>
                  <img src='images/gulp_depth.jpg'>
                </div>
                <script type="text/javascript">
                  function gulp_start() {
                    document.getElementById('cauli_image').style.opacity = "1";
                  }
                  function gulp_stop() {
                    document.getElementById('cauli_image').style.opacity = "0";
                  }
                  gulp_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2210.06545pdf">
                  <papertitle>GULP: a prediction-based metric between representations</papertitle>
                </a>
                <br>
                <a href="http://web.mit.edu/eboix/www/">Enric Boix-Adsera</a>,
                <strong>Hannah Lawrence</strong>,
                <a href="https://scholar.google.com/citations?user=CKYZLxYAAAAJ&hl=en">George Stepaniants</a>,
                <a href="https://math.mit.edu/~rigollet/">Philippe Rigollet</a>
                <br>
                 <em>NeurIPS (Oral Presentation, to appear)</em>, 2022 
                <br>
                <p></p>
                <p>We define a family of distance pseudometrics for comparing learned data representations, directly inspired by transfer learning. In particular, we define a distance between two representations based on how differently (worst-case over all downstream, bounded linear predictive tasks) they perform under ridge regression.</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="barron_stop()" onmouseover="barron_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='barron_image'><img src='images/sphere_two.jpg'></div>
                  <img src='images/sphere_one.jpg'>
                </div>
                <script type="text/javascript">
                  function barron_start() {
                    document.getElementById('barron_image').style.opacity = "1";
                  }
                  function barron_stop() {
                    document.getElementById('barron_image').style.opacity = "0";
                  }
                  barron_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2210.06545pdf">
                  <papertitle>Barron's Theorem for Equivariant Networks</papertitle>
                </a>
                <br>
                <strong>Hannah Lawrence</strong>
                <br>
                 <em>NeurIPS Workshop: <a href="https://www.neurreps.org/">Symmetry and Geometry in Neural Representations</a> (Poster, to appear)</em>, 2022 
                <br>
                <p></p>
                <p>We extend Barron’s Theorem for efficient approximation to invariant neural networks, in the cases of invariance to a permutation subgroup or the rotation group.</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="sublinear_stop()" onmouseover="sublinear_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sublinear_image'><img src='images/toep_noisy.jpg'></div>
                  <img src='images/toep_clean.jpg'>
                </div>
                <script type="text/javascript">
                  function sublinear_start() {
                    document.getElementById('sublinear_image').style.opacity = "1";
                  }
                  function sublinear_stop() {
                    document.getElementById('sublinear_image').style.opacity = "0";
                  }
                  sublinear_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://soda2023.hotcrp.com/paper/243?cap=hcav243UHrkyGVtawupysKqfQoMjeHb">
                  <papertitle>Toeplitz Low-Rank Approximation with Sublinear Query Complexity</papertitle>
                </a>
                <br>
                <a href="https://theory.epfl.ch/kapralov/">Michael Kapralov</a>,
                <strong>Hannah Lawrence</strong>,
                <a href="https://people.epfl.ch/mikhail.makarov?lang=en">Mikhail Makarov</a>,
                <a href="https://people.cs.umass.edu/~cmusco/">Cameron Musco</a>,
                <a href="https://ksheth96.github.io/">Kshiteej Sheth</a>
                <br>
                 <em>Symposium on Discrete Algorithms (SODA), to appear</em>, 2023
                <br>
                <p></p>
                <p>We present a framework for automatically identifying and captioning coherent patterns of errors made by any trained model. The key? Keeping it simple: linear classifiers in a shared vision-language embedding space.</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="distill_stop()" onmouseover="distill_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='distill_image'><img src='images/with_svm.jpg'></div>
                  <img src='images/without_svm.jpg'>
                </div>
                <script type="text/javascript">
                  function distill_start() {
                    document.getElementById('distill_image').style.opacity = "1";
                  }
                  function distill_stop() {
                    document.getElementById('distill_image').style.opacity = "0";
                  }
                  distill_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2206.14754.pdf">
                  <papertitle>Distilling Model Failures as Directions in Latent Space</papertitle>
                </a>
                <br>
                <a href="http://people.csail.mit.edu/saachij/">Saachi Jain<sup>*</sup></a>,
                <strong>Hannah Lawrence<sup>*</sup></strong>,
                <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a>,
                <a href="https://madry.mit.edu/">Aleksander Madry</a>
                <br>
                 <em>In submission</em>, 2022 
                <br>
                <p></p>
                <p>We present a framework for automatically identifying and captioning coherent patterns of errors made by any trained model. The key? Keeping it simple: linear classifiers in a shared vision-language embedding space.</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="bias_stop()" onmouseover="bias_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='bias_image'><img src='images/web_bias_lower.jpg'></div>
                  <img src='images/web_bias_upper.jpg'>
                </div>
                <script type="text/javascript">
                  function bias_start() {
                    document.getElementById('bias_image').style.opacity = "1";
                  }
                  function bias_stop() {
                    document.getElementById('bias_image').style.opacity = "0";
                  }
                  bias_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2110.06084.pdf">
                  <papertitle>Implicit Bias of Linear Equivariant Networks</papertitle>
                </a>
                <br>
                <strong>Hannah Lawrence</strong>,
                <a href="https://kristian-georgiev.github.io/">Kristian Georgiev</a>,
                <a href="https://www.linkedin.com/in/andrew-dienes-83981914a">Andrew Dienes</a>,
                <a href="https://scholar.google.com/citations?user=fz1mq4AAAAAJ&hl=en">Bobak T. Kiani<sup>*</sup></a>
                <br>
                 <em>Appearing at ICML</em>, 2022 
                <br>
                <p></p>
                <p>We characterize the implicit bias of linear group-convolutional networks trained by gradient descent. In particular, we show that the learned linear function is biased towards low-rank matrices in Fourier space.</p>
              </td>
            </tr>
          </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="phase_stop()" onmouseover="phase_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='phase_image'><img src='images/phase_after.png'></div>
                <img src='images/phase_before.png'>
              </div>
              <script type="text/javascript">
                function phase_start() {
                  document.getElementById('phase_image').style.opacity = "1";
                }
                function phase_stop() {
                  document.getElementById('phase_image').style.opacity = "0";
                }
                phase_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.07386">
                <papertitle>Phase Retrieval with Holography and Untrained Priors: Tackling the Challenges of Low-Photon Nanoscale Imaging</papertitle>
              </a>
              <br>
              <strong>Hannah Lawrence <sup>*</sup> </strong>,
              <a href="https://davidbar.org/">David A. Barmherzig <sup>*</sup></a>,
              <a href="https://math.yale.edu/people/henry-li">Henry Li</a>,
              <a href="https://eickenberg.github.io/">Michael Eickenberg</a>,
              <a href="https://marylou-gabrie.github.io/">Marylou Gabrié</a>
              <br>
               <em>Appeared at MSML</em>, 2021
              <br>
              <p></p>
              <p>By using a maximum-likelihood objective coupled with a deep decoder prior for images, we achieve superior image reconstruction for holographic phase retrieval, including under several challenging realistic conditions. To our knowledge, this is the first dataset-free machine learning approach for holographic phase retrieval. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="online_stop()" onmouseover="online_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='online_image'><img src='images/online_after.jpg'></div>
                <img src='images/online_before.jpg'>
              </div>
              <script type="text/javascript">
                function online_start() {
                  document.getElementById('online_image').style.opacity = "1";
                }
                function online_stop() {
                  document.getElementById('online_image').style.opacity = "0";
                }
                online_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.10873">
                <papertitle>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</papertitle>
              </a>
              <br>
              <a href="http://campuspress.yale.edu/lchen/">Lin Chen</a>,
              <a href="https://sites.google.com/usc.edu/qyu/home">Qian Yu</a>,
              <strong>Hannah Lawrence</strong>,
              <a href="https://seas.yale.edu/faculty-research/faculty-directory/amin-karbasi">Amin Karbasi</a>
              <br>
               <em>Appeared at NeurIPS</em>, 2020
              <br>
              <p></p>
              <p>We establish the minimax regret of switching-constrained online convex optimization, a realistic optimization framework where algorithms must act in real-time to minimize cumulative loss, but are penalized if they are too erratic.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="toeplitz_stop()" onmouseover="toeplitz_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='toeplitz_image'><img src='images/toeplitz_after.jpg'></div>
                <img src='images/toeplitz_before.jpg'>
              </div>
              <script type="text/javascript">
                function toeplitz_start() {
                  document.getElementById('toeplitz_image').style.opacity = "1";
                }
                function toeplitz_stop() {
                  document.getElementById('toeplitz_image').style.opacity = "0";
                }
                toeplitz_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1911.08015">
                <papertitle>Low-Rank Toeplitz Matrix Estimation via Random Ultra-Sparse Rulers</papertitle>
              </a>
              <br>
              <strong>Hannah Lawrence</strong>,
              <a href="https://jerryzli.github.io/">Jerry Li</a>,
              <a href="https://people.cs.umass.edu/~cmusco/">Cameron Musco</a>,
              <a href="https://www.chrismusco.com/">Christopher Musco</a>
              <br>
               <em>Appeared at ICASSP</em>, 2020
              <br>
              <p></p>
              <p>By building new, randomized "ruler" sampling constructions, we show how to use sublinear sparse Fourier transform algorithms for sample efficient, low-rank, Toeplitz covariance estimation.</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/YaleUniversityLogo.jpg"></td>
            <td width="75%" valign="center">
              Hertz Foundation Summer Workshop Committee, Fall 2021 and Spring 2022
              <br>
              <br>
              Women in Learning Theory Mentor, Spring 2020
              <br>
              <br>
              Applied Math Departmental Student Advisory Committee, Spring 2019
              <br>
              <br>
              Dean's Committee on Science and Quantitative Reasoning, Fall 2018
              <br>
              <br>
              Undergraduate Learning Assistant, CS 365 (Design and Analysis of Algorithms), Spring 2018
              <br>
              <br>
              Undergraduate Learning Assistant, CS 223 (Data Structures and Algorithms), Spring 2017
              <br>
              <br>
              Undergraduate Learning Assistant, CS 201 (Introduction to Computer Science), Fall 2017
            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website template credits.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
